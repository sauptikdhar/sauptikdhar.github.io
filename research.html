<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Sauptik Dhar</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Sauptik Dhar</div>
<div class="menu-item"><a href="index.html">home</a></div>
<div class="menu-item"><a href="research.html" class="current">research&nbsp;career</a></div>
<div class="menu-item"><a href="publications.html">publications</a></div>
<div class="menu-category">Software Downloads</div>
<div class="menu-item"><a href="univClass.html">Universum&nbsp;Learning</a></div>
<div class="menu-item"><a href="ADMML.html">ADMML</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Research Career</h1>
</div>
<h2>Work Experience</h2>
<ol>
<li><p><b>LG Electronics</b> (Feb 2018 - Current) <b>Position</b> AI Technical Lead.</p>
</li>
<li><p><b>Robert Bosch LLC</b> <br />
(Jan 2016 - Jan 2018): Senior Research Scientist. <br />
(Dec 2013 - Jan 2016): Research Scientist. <br />
(May 2013 - Sept 2013): Research Intern. <br /></p>
</li>
<li><p><b>University of Minnesota</b> (Aug 2008 - Dec 2013) <b>Position</b>: Research Assistant/Teaching Assistant.</p>
</li>
<li><p><b>Wipro Technologies</b> (June 2006 - May 2008) <b>Position</b>: Software Engineer (Java/ Unix).</p>
</li>
</ol>
<p><b>Mentored Internships</b> <br />
I had the chance to interact with brilliant and wonderful researchers during the last 8 years of my career. Check out their work below!</p>
<ul>
<li><p>Congrui Yi (Summer 2014). (Current Position: Applied Scientist at Amazon) </p>
</li>
<li><p>Vikas Bhetanabhotla (Summer 2015) (Current Position: SW Engg. at Cloud Security) </p>
</li>
<li><p>Julian Walbrecht (Fall 2015) (Current Position: Connected Industry SW Engg.) </p>
</li>
<li><p>Goutham Kamath (Summer 2016) (Current Position: Research Scientist at Foghorn Systems) </p>
</li>
<li><p>Youngsuk Park (Summer 2017) (Current Position: AWS AI Labs) </p>
</li>
<li><p>Bernardo Torres (Summer 2017) (Current Position: PhD at UCSC) </p>
</li>
</ul>
<h2>Other Professional Activities</h2>
<ul>
<li><p>Associate Editor of <a href="http://link.springer.com/journal/11063">Neural Processing Letters</a> (2016-current)</p>
</li>
<li><p>Program Committee: KDD (2016 - 2020), SDM 2018, ICMLA (2016 - 2020), HIS 2017, Baylearn 2017, ICPRAI 2018. </p>
</li>
<li><p>Reviewer for Neural Networks, Pattern Recognition, Neurocomputing, PLoS ONE, Neural Processing Letters, IEEE Systems Man and Cybernetics</p>
</li>
<li><p>Reviewer for International Joint Conference on Neural Networks (IJCNN 2009-2016), NIPS 2016.</p>
</li>
</ul>
<h2>Current Projects</h2>
<h3>Learning on Edge Devices (On Device Learning, TinyML)</h3>
<table class="imgtable"><tr><td>
<img src="files/edge.jpg" alt="edge" width="300px" height="200px" />&nbsp;</td>
<td align="left"><p>With the increase in number of smart devices and the compute power on these devices; there is interest in performing model training on the edge devices. In this research we analyze the current landscape of the edge learning technology and develop new methods to enable model training with reduced resource (memory, compute, energy) footprint.</p>
<p><b>Selected Publications</b>: <br /></p>
<ul>
<li><p>Sauptik Dhar, Junyao Guo, Jiayi Liu, Samarth Tripathi, Unmesh Kurup, Mohak Shah, <a href="https://arxiv.org/abs/1911.00623">&ldquo;On-Device Machine Learning: An Algorithms and Learning Theory Perspective&rdquo;</a>, ACM TIOT, 2021 (to appear). </p>
</li>
</ul>
</td></tr></table>
<h3>Deep Learning Hyperparameter (AutoML) and Model Optimization. </h3>
<table class="imgtable"><tr><td>
<img src="files/hpo.jpg" alt="hpo" width="300px" height="200px" />&nbsp;</td>
<td align="left"><p>The success of any machine learning (including deep-learning) algorithm depends on careful tuning of its hyperparameters. However hyperparameter tuning is a non-trivial problem and has been a topic of research for several decades. In this research we develop advanced Hyperparameter optimization algorithms which provides significant speed-ups compared to existing state-of-art algorithms. A part of this research also focus on advanced optimization strategies for Deep Learning. <br /> 
<b>Selected Publications</b>: <br /></p>
<ul>
<li><p>S.Dhar, U.Kurup, M.Shah, <a href="https://www.automl.org/wp-content/uploads/2020/07/AutoML_2020_paper_24.pdf">“Stabilizing Bi-Level Hyperparameter Optimization using Moreau-Yosida Regularization,”</a> <a href="https://www.automl.org/wp-content/uploads/2020/07/AutoML_2020_paper_24_poster.pdf">(Poster)</a> ICML 2020 (AutoML Workshop).</p>
</li>
<li><p>S.Dhar, U.Kurup, M.Shah, “Moreau-Yosida Regularized Bi-Level Hyperparameter Optimization,” KDD 2020 (AutoML Workshop).</p>
</li>
</ul>
</td></tr></table>
<h3>Generative Adversarial Networks (GANS)</h3>
<p>Coming Soon!</p>
<h3>Learning from Contradictions (Universum Learning)</h3>
<table class="imgtable"><tr><td>
<img src="files/usvm.jpg" alt="usvm" width="300px" height="200px" />&nbsp;</td>
<td align="left"><p>The technique of Universum learning provides a formal mechanism for incorporating a priori knowledge about the application domain, in the form of additional Universum samples. Universum learning has been originally introduced for binary classification (by <a href="http://ronan.collobert.com/pub/matos/2006_universum_icml.pdf">Vapnik</a>) and works specifically well for limited data settings. In this project we formalize Universum Learning to newer learning problems like, regression, multi-class, single-class (anomaly) detection etc. </p>
<p><b>Selected Publications</b>: <br /></p>
<ul>
<li><p>S. Dhar, V.Cherkassky, M. Shah, &ldquo;<a href="https://papers.nips.cc/paper/9048-multiclass-learning-from-contradictions">Multiclass Learning from Contradictions</a>&rdquo;, NeuRIPS 2019.</p>
</li>
<li><p>S. Dhar, V. Cherkassky, &ldquo;<a href="files/cost_sensitiveusvm_journal.pdf">Development and Evaluation of Cost-Sensitive Universum SVM</a>&rdquo;, <i>IEEE Transactions on
Systems, MAN, and Cybernetics PART B: Cybernetics</i>, vol. 45, no. 4, pp. 806-817, Apr 2015.</p>
</li>
<li><p>V. Cherkassky, S. Dhar, and W. Dai, &ldquo;<a href="files/tnn_USVM.pdf">Practical Conditions for Effectiveness of the Universum Learning</a>,&rdquo; <i>IEEE
Transactions on Neural Networks</i>, vol.22, no. 8, pp. 1241-1255, Aug 2011.</p>
</li>
</ul>
</td></tr></table>
<h2>Past Projects</h2>
<h3>Explainable AI Models</h3>
<table class="imgtable"><tr><td>
<img src="files/vis.jpg" alt="vis" width="300px" height="200px" />&nbsp;</td>
<td align="left"><p>Many machine learning applications involve predictive data-analytic modeling using black-box techniques. A common problem in such studies is understanding/interpretation of the estimated models. This project analyzes the current techniques for visualization and interpretation of black-box models and their applicability under VC-theoretic framework.</p>
<p><b>Selected Publications</b>: <br /></p>
<ul>
<li><p>V. Cherkassky, S. Dhar, &ldquo;<a href="http://link.springer.com/chapter/10.1007%2F978-3-319-21852-6_19">Interpretation of Black-Box Predictive Models</a>&rdquo;. <i>Measures of Complexity: Festschrift for
Alexey Chervonenkis</i> (Editors: V. Vovk , H. Papadopoulos, A. Gammerman), Oct 2015. ( <a href="files/Visualization_and_Interpretation_of_SVM_Classifiers.pdf">other versions</a>)</p>
</li>
<li><p>V. Cherkassky, S. Dhar, &ldquo;<a href="files/simple_method_for_interpretation.pdf">Simple Method for Interpretation of High-Dimensional Nonlinear SVM Classification
Models</a>&rdquo;, <i>Proceedings of the 2010 International Conference on Data Mining</i>, July 2010. </p>
</li>
</ul>
</td></tr></table>
<h3>Big Data and Scalable ML algorithms (at BOSCH Research)</h3>
<table class="imgtable"><tr><td>
<img src="files/bigdata_text.jpg" alt="Big_data" width="300px" height="200px" />&nbsp;</td>
<td align="left"><p>With the advent of big-data many data-mining applications involves analyzing huge amount of data (e.g millions of records and <i></i> or thousands of variables) and derive insights from it. This necessiates research in large-scale data analysis <i>systems</i> and <i>algorithms</i>. In this project we analyze and extend newer optimization algorithms (as scale) on existing big-data frameworks. The outcome of this research will provide tools for building scalable ML algorithms suitable for deriving insights from big-data. </p>
<p><b>Selected Publications</b>: <br /></p>
<ul>
<li><p>N. Ramakrishnan, S. Dhar, J. Irion, Scalable Graph SLAM for HD Maps, US Patent App.</p>
</li>
<li><p>S. Dhar, C. Yi, N. Ramakrishnan, and M. Shah, &ldquo;<a href="files/final_paper_ieeebigdataADMM.pdf">ADMM based Scalable Machine Learning on Spark</a>&rdquo;, IEEE Big Data 2015 , Santa Clara, 2015. (<a href="https://databricks.com/session/admm-based-scalable-machine-learning-on-apache-spark">Spark Summit Talk</a>)</p>
</li>
<li><p>Y. Park, S.Dhar, S. Boyd, M. Shah, &ldquo;<a href="files/nips_vmpg.pdf">Variable Metric Proximal Gradient Method with Diagonal Barzilai-Borwein Stepsize</a>&rdquo;, NIPS workshop on <a href="http://opt-ml.org/papers.html">Optimization for Machine Learning</a>, 2017</p>
</li>
</ul>
</td></tr></table>
<h3>Use-Case Research (at BOSCH Research)</h3>
<table class="imgtable"><tr><td>
<img src="files/app.jpg" alt="app" width="300px" height="200px" />&nbsp;</td>
<td align="left"><p>As a member of the Data Science Team we are involved in various projects towards deriving insights from the data available in various domains such as., <a href="http://www.bosch.us/en/us/startpage_1/country-landingpage.php"><i>Automotive</i>, <i>Manufacturing</i>, <i>HealthCare</i>, <i>ThermoTechnology</i></a> etc. A successful project life-cycle spans from requirements gathering to deployment.</p>
<p><b>Selected Publications</b>: <br /></p>
<ul>
<li><p>M.Ganser, S. Dhar, U. Kurup, C.Cunha, and A.Gacic, &ldquo;<a href="files/final_paper_ieee_bigdataclieff_ieeepdfexpress.pdf">A Data-Driven approach towards Patient Identification for
Tele-health Programs</a>&rdquo;, <i>IEEE Big Data 2015</i> (Workshop on Mining Big Data to Improve Clinical Effectiveness). </p>
</li>
</ul>
</td></tr></table>
<h3>Statistical Analysis of the Soil Chemical Survey Data (with MNDoT)</h3>
<table class="imgtable"><tr><td>
<img src="files/Soil.jpg" alt="Soil" width="300px" height="200px" />&nbsp;</td>
<td align="left"><p>This project describes the data-analytic modeling of the Minnesota soil chemical data produced by the 2001 metro soil survey and by the 2003 state-wide survey. The chemical composition of the soil is characterized by the concentration of many metal and non-metal constituents, resulting in high-dimensional data. This high dimensionality and possible unknown (nonlinear) correlations in the data make it difficult to analyze and interpret using standard statistical techniques. This project applies a machine learning technique, called Self Organizing Map (SOM), to present the high-dimensional soil data in a 2D format suitable for human understanding and interpretation. This SOM representation enables analysis of the soil chemical concentration trends within the metro area and in the state of Minnesota. These trends are important for various Minnesota regulatory agencies concerned with the concentration of polluting chemical elements due to both (a) human activities, i.e., different industrial land usage, and (b) natural geological factors, such as the geomorphic codes and provenance of glacial sediments</p>
<p><b>Selected Publications</b>: <br /></p>
<ul>
<li><p>S. Dhar, V. Cherkassky, &ldquo;<a href="http://www.lrrb.org/media/reports/201022.pdf">Statistical Analysis of the Soil Chemical Survey Data</a>&rdquo;, Report no. Mn/DOT 2010-22, June
2010. ( Technical Report)</p>
</li>
<li><p>S. Dhar, V. Cherkassky, &ldquo;<a href="files/final_paperIJCNN.pdf">Application of SOM to Analysis of Minnesota Soil Survey Data</a>&rdquo;, <i>International Joint
Conference on Neural Networks (IJCNN)</i>, Feb 2011.</p>
</li>
</ul>
</td></tr></table>
<h2>Other (Minor) Projects / Course Projects</h2>
<ul>
<li><p><a href="files/minor/csci5461.pdf">Discriminative Feature selection for Remote Homology detection using sparse coding</a> </p>
</li>
<li><p><a href="files/csci5525.pdf">Practical Conditions for Effectiveness of the Universum Learning</a> </p>
</li>
<li><p><a href="files/minor/ee4541.pdf">Emotion Recognition from Speech Signals</a> </p>
</li>
<li><p><a href="files/minor/ee5239.pdf">A Novel Framework for Classification using Regression routines</a> </p>
</li>
<li><p><a href="files/minor/ee5561.ppt">Transfer Learning for Image Classification</a> </p>
</li>
<li><p><a href="files/minor/ee8591.pdf">Development and Evaluation of Learning Softwares</a> </p>
</li>
<li><p><a href="files/minor/ee5931.pdf">Practical Conditions for Effectiveness of the Universum Learning</a></p>
</li>
<li><p><a href="files/minor/ee8932.ppt">Practical Conditions for Effectiveness of the Universum Learning</a></p>
</li>
</ul>
<div class="infoblock">
<div class="blockcontent">
<p><b>Disclaimer</b>: The views and opinions expressed in this website are strictly those of the site's author. </p>
</div></div>
<div id="footer">
<div id="footer-text">
Page generated 2021-03-25 01:15:21 Pacific Daylight Time, by <a href="http://jemdoc.jaboc.net/">jemdoc</a>.
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
